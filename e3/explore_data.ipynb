{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from dataset.dataset import DNADataset\n",
    "from model.origami_model import DNAOrigamiModel\n",
    "from model.transformer import EncoderLayer, TransformerEncoder\n",
    "# from model.origami_model import DNAOrigamiModel\n",
    "from utils.logger import get_logger\n",
    "from utils.parsing import parse_dna_origami_data\n",
    "from utils.tokenize import tokenize_trajectory\n",
    "from oxDNA_analysis_tools.UTILS.RyeReader import describe, get_confs, inbox\n",
    "import numpy as np\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 86, 13])\n",
      "torch.Size([2, 3655])\n",
      "torch.Size([4, 86, 86])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: The old topology format is depreciated and future tools may not support it.  Please update to the new topology format for future simulations.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 45 is out of bounds for dimension 0 with size 4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m trajectory_filepaths \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset/data/trajectory.dat\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      2\u001b[0m topology_filepaths \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset/data/output.top\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m----> 4\u001b[0m combined_data \u001b[38;5;241m=\u001b[39m \u001b[43mtokenize_trajectory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparse_dna_origami_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrajectory_filepaths\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopology_filepaths\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m combined_data\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# dataset = DNADataset(trajectory_filepaths, topology_filepaths)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/matthew/project_files/dnaOrigami/12_4_2024_zip_files/dna_origami/utils/tokenize.py:61\u001b[0m, in \u001b[0;36mtokenize_trajectory\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     57\u001b[0m pairwise_dist_matrix \u001b[38;5;241m=\u001b[39m pairwise_distances(positions)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28mprint\u001b[39m(pairwise_dist_matrix\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 61\u001b[0m edge_attr_distances \u001b[38;5;241m=\u001b[39m \u001b[43mpairwise_dist_matrix\u001b[49m\u001b[43m[\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# add angles as edge features\u001b[39;00m\n\u001b[1;32m     64\u001b[0m edge_attr_angles \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mIndexError\u001b[0m: index 45 is out of bounds for dimension 0 with size 4"
     ]
    }
   ],
   "source": [
    "trajectory_filepaths = [\"dataset/data/trajectory.dat\"]\n",
    "topology_filepaths = [\"dataset/data/output.top\"]\n",
    "\n",
    "combined_data = tokenize_trajectory(parse_dna_origami_data(trajectory_filepaths[0], topology_filepaths[0]))\n",
    "\n",
    "combined_data.shape\n",
    "\n",
    "# dataset = DNADataset(trajectory_filepaths, topology_filepaths)\n",
    "# dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: The old topology format is depreciated and future tools may not support it.  Please update to the new topology format for future simulations.\n"
     ]
    }
   ],
   "source": [
    "trajectory_filepaths = [\"dataset/data/trajectory.dat\"]\n",
    "topology_filepaths = [\"dataset/data/output.top\"]\n",
    "\n",
    "top_info, traj_info = describe(topology_filepaths[0], trajectory_filepaths[0])\n",
    "n_confs = traj_info.nconfs\n",
    "start_conf = 0\n",
    "confs = get_confs(top_info, traj_info, start_conf, n_confs)\n",
    "\n",
    "confs = [inbox(conf, center=True) for conf in confs]\n",
    "\n",
    "pos = [conf.positions for conf in confs]\n",
    "a1 = [conf.a1s for conf in confs]\n",
    "a3 = [conf.a3s for conf in confs]\n",
    "\n",
    "n_particles = pos[0].shape[0]\n",
    "trajectory_data = np.array([pos, a1, a3]).reshape(n_confs,n_particles, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset.graphs[0]['nodes']\n",
    "data = data.reshape(1, data.shape[0], data.shape[1])\n",
    "\n",
    "attn_mask = torch.ones(data.shape[0], data.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = nn.Linear(13, 64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 86, 64])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed(data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {'num_layers':1, 'n_features':13, 'd_model': 64, 'nhead': 1, 'num_encoder_layers': 6, 'd_ff': 256, 'dropout_rate': 0.1, 'device': 'cpu'}\n",
    "encoder = TransformerEncoder(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.5675, -0.0164, -0.7847,  ...,  0.0181, -0.1691, -0.5160],\n",
       "         [ 0.3918, -0.0264, -0.2521,  ..., -0.1603, -0.3439, -0.4047],\n",
       "         [ 0.6419, -0.2373, -0.1428,  ...,  0.3467, -0.3449,  0.2875],\n",
       "         ...,\n",
       "         [ 1.0128,  1.4064,  1.0862,  ..., -0.5838, -1.1544,  1.9007],\n",
       "         [ 0.6751,  1.3832,  1.1350,  ..., -0.5791, -1.1484,  1.9547],\n",
       "         [-0.5929,  0.1272,  0.1006,  ..., -0.5408, -0.2063,  1.6927]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder(data, attn_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3655])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset.graphs[0]['nodes']\n",
    "\n",
    "dataset.graphs[0]['edge_index'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'img' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m encoder(\u001b[43mimg\u001b[49m, return_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'img' is not defined"
     ]
    }
   ],
   "source": [
    "encoder(img, return_embeddings = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "EinopsError",
     "evalue": " Error while processing rearrange-reduction pattern \"b c (h p1) (w p2) -> b (h w) (p1 p2 c)\".\n Input tensor shape: torch.Size([1, 86, 13]). Additional info: {'p1': 43, 'p2': 43}.\n Wrong shape: expected 4 dims. Received 3-dim tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEinopsError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/micromamba/envs/aptamer_transformer/lib/python3.11/site-packages/einops/einops.py:522\u001b[0m, in \u001b[0;36mreduce\u001b[0;34m(tensor, pattern, reduction, **axes_lengths)\u001b[0m\n\u001b[1;32m    521\u001b[0m shape \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39mshape(tensor)\n\u001b[0;32m--> 522\u001b[0m recipe \u001b[38;5;241m=\u001b[39m \u001b[43m_prepare_transformation_recipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43maxes_lengths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mndim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _apply_recipe(\n\u001b[1;32m    524\u001b[0m     backend, recipe, cast(Tensor, tensor), reduction_type\u001b[38;5;241m=\u001b[39mreduction, axes_lengths\u001b[38;5;241m=\u001b[39mhashable_axes_lengths\n\u001b[1;32m    525\u001b[0m )\n",
      "File \u001b[0;32m~/micromamba/envs/aptamer_transformer/lib/python3.11/site-packages/einops/einops.py:365\u001b[0m, in \u001b[0;36m_prepare_transformation_recipe\u001b[0;34m(pattern, operation, axes_names, ndim)\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ndim \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(left\u001b[38;5;241m.\u001b[39mcomposition):\n\u001b[0;32m--> 365\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m EinopsError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrong shape: expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(left\u001b[38;5;241m.\u001b[39mcomposition)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m dims. Received \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-dim tensor.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    366\u001b[0m left_composition \u001b[38;5;241m=\u001b[39m left\u001b[38;5;241m.\u001b[39mcomposition\n",
      "\u001b[0;31mEinopsError\u001b[0m: Wrong shape: expected 4 dims. Received 3-dim tensor.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mEinopsError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m img \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mgraphs[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnodes\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m86\u001b[39m, \u001b[38;5;241m13\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# caption = torch.randint(0, 20000, (1, 1024))\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m encoded \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_embeddings\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# decoder(caption, context = encoded) # (1, 1024, 20000)\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/aptamer_transformer/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/aptamer_transformer/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/aptamer_transformer/lib/python3.11/site-packages/x_transformers/x_transformers.py:1529\u001b[0m, in \u001b[0;36mViTransformerWrapper.forward\u001b[0;34m(self, img, return_embeddings, return_logits_and_embeddings)\u001b[0m\n\u001b[1;32m   1521\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m   1522\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1523\u001b[0m     img,\n\u001b[1;32m   1524\u001b[0m     return_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1525\u001b[0m     return_logits_and_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1526\u001b[0m ):\n\u001b[1;32m   1527\u001b[0m     b, p \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_size\n\u001b[0;32m-> 1529\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mrearrange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mb c (h p1) (w p2) -> b (h w) (p1 p2 c)\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp1\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1530\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_to_embedding(x)\n\u001b[1;32m   1531\u001b[0m     n \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/micromamba/envs/aptamer_transformer/lib/python3.11/site-packages/einops/einops.py:591\u001b[0m, in \u001b[0;36mrearrange\u001b[0;34m(tensor, pattern, **axes_lengths)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrearrange\u001b[39m(tensor: Union[Tensor, List[Tensor]], pattern: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39maxes_lengths) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    537\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;124;03m    einops.rearrange is a reader-friendly smart element reordering for multidimensional tensors.\u001b[39;00m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;124;03m    This operation includes functionality of transpose (axes permutation), reshape (view), squeeze, unsqueeze,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    589\u001b[0m \n\u001b[1;32m    590\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 591\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrearrange\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43maxes_lengths\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/aptamer_transformer/lib/python3.11/site-packages/einops/einops.py:533\u001b[0m, in \u001b[0;36mreduce\u001b[0;34m(tensor, pattern, reduction, **axes_lengths)\u001b[0m\n\u001b[1;32m    531\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Input is list. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    532\u001b[0m message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdditional info: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(axes_lengths)\n\u001b[0;32m--> 533\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m EinopsError(message \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(e))\n",
      "\u001b[0;31mEinopsError\u001b[0m:  Error while processing rearrange-reduction pattern \"b c (h p1) (w p2) -> b (h w) (p1 p2 c)\".\n Input tensor shape: torch.Size([1, 86, 13]). Additional info: {'p1': 43, 'p2': 43}.\n Wrong shape: expected 4 dims. Received 3-dim tensor."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from x_transformers import ViTransformerWrapper, TransformerWrapper, Encoder, Decoder\n",
    "\n",
    "encoder = ViTransformerWrapper(\n",
    "    image_size = 86,\n",
    "    patch_size = 43,\n",
    "    attn_layers = Encoder(\n",
    "        dim = 512,\n",
    "        depth = 6,\n",
    "        heads = 8\n",
    "    )\n",
    ")\n",
    "\n",
    "# decoder = TransformerWrapper(\n",
    "#     num_tokens = 20000,\n",
    "#     max_seq_len = 1024,\n",
    "#     attn_layers = Decoder(\n",
    "#         dim = 512,\n",
    "#         depth = 6,\n",
    "#         heads = 8,\n",
    "#         cross_attend = True\n",
    "#     )\n",
    "# )\n",
    "\n",
    "img = torch.randn(1, 3, 256, 256)\n",
    "img = dataset.graphs[0]['nodes'].reshape(1, 86, 13)\n",
    "# caption = torch.randint(0, 20000, (1, 1024))\n",
    "\n",
    "encoded = encoder(img, return_embeddings = True)\n",
    "# decoder(caption, context = encoded) # (1, 1024, 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.graphs[0]['nodes'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = dataset.graphs[0]['nodes'][:, :3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming pos is already defined\n",
    "# pos = dataset.graphs[0]['nodes'][:, :3]\n",
    "\n",
    "# Convert pos to a DataFrame for easier plotting\n",
    "df = pd.DataFrame(pos, columns=['x', 'y', 'z'])\n",
    "\n",
    "# Create a 3D scatter plot\n",
    "fig = px.scatter_3d(df, x='x', y='y', z='z')\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
