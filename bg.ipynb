{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aaf11f34-e308-45ac-b984-f3065fac553b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96712572-b4f1-4c18-93ec-38accd0fff42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AffineCouplingLayer(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=512):\n",
    "        super(AffineCouplingLayer, self).__init__()\n",
    "        split_dim = input_dim // 2 \n",
    "        if input_dim % 2 !=0:\n",
    "            split_dim += 1\n",
    "            \n",
    "        self.hidden_dim = hidden_dim\n",
    "        print(f\"hidden dim{self.hidden_dim}\")\n",
    "\n",
    "        # Added skip connections\n",
    "        self.scale_net_fc1 = nn.Linear(split_dim, self.hidden_dim)\n",
    "        self.scale_net_fc2 = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        self.scale_net_fc3 = nn.Linear(self.hidden_dim, split_dim)\n",
    "\n",
    "        # Added skip connections\n",
    "        self.translate_net_fc1 = nn.Linear(split_dim, self.hidden_dim)\n",
    "        self.translate_net_fc2 = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        self.translate_net_fc3 = nn.Linear(self.hidden_dim, split_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # splitting input into 2 chunks\n",
    "        # print(f\"Input shape: {x.shape}\")\n",
    "        split_dim = x.size(1) // 2\n",
    "        print(f\"split dim:{split_dim}\")\n",
    "        print(f\"x{x.shape}\")\n",
    "        if x.size(1) % 2 !=0:\n",
    "            split_dim += 1\n",
    "            \n",
    "        x1 = x[:, :split_dim]\n",
    "        x2 = x[:, split_dim:]\n",
    "        print(f\"x1{x1.shape}\")\n",
    "        print(f\"x2{x2.shape}\")\n",
    "        # x1, x2 = x.chunk(2, dim=1)\n",
    "\n",
    "        # print(f\"x1 shape: {x1.shape}, x2 shape: {x2.shape}\")\n",
    "        # Scale network with skip connection\n",
    "        hidden = torch.relu(self.scale_net_fc1(x1))\n",
    "        hidden = torch.relu(self.scale_net_fc2(hidden))+hidden # skip connection\n",
    "        scale = self.scale_net_fc3(hidden)\n",
    "\n",
    "        # Translate network with skip connection\n",
    "        hidden = torch.relu(self.translate_net_fc1(x1))\n",
    "        hidden = torch.relu(self.translate_net_fc2(hidden))+hidden # skip connection\n",
    "        translate = self.translate_net_fc3(hidden)\n",
    "        \n",
    "        scale = torch.clamp(scale, min=-5.0, max=5.0)\n",
    "        # Apply affine transformations to x2\n",
    "        z2 = x2 * torch.exp(scale) + translate\n",
    "        print(f\"{x1.shape()}\")\n",
    "        print(f\"{x2.shape()}\")\n",
    "        # output = torch.cat([x1[:, :z2.size(1)], z2], dim=1)\n",
    "        return torch.cat([x1, z2], dim=1), scale\n",
    "\n",
    "    def inverse(self, z):\n",
    "        split_dim = z.size(1) // 2\n",
    "        z1 = z[:, :split_dim]\n",
    "        z2 = z[:, split_dim:]\n",
    "        # z1, z2 = z.chunk(2, dim=1)\n",
    "\n",
    "        # Scale network with skip connection\n",
    "        hidden = torch.relu(self.scale_net_fc1(z1))\n",
    "        hidden = torch.relu(self.scale_net_fc2(hidden))+hidden\n",
    "        scale = self.scale_net_fc3(hidden)\n",
    "\n",
    "        # Translate network with skip connection\n",
    "        hidden = torch.relu(self.translate_net_fc1(z1))\n",
    "        hidden = torch.relu(self.translate_net_fc2(hidden))+hidden\n",
    "        translate = self.translate_net_fc3(hidden)\n",
    "        \n",
    "        scale = torch.clamp(scale, min=-5.0, max=5.0)\n",
    "        # Inverse affine transformation\n",
    "        x2 = (z2 - translate) * torch.exp(-scale)\n",
    "        return torch.cat([z1, x2], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15d7e00a-b512-46fa-a4a5-f9d72cab667d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RealNVP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers):\n",
    "        super(RealNVP, self).__init__()\n",
    "        # if input_dim % 2 != 0:\n",
    "        #     raise ValueError(\"Input dimension must be even for chunking.\")\n",
    "        # self.layers = nn.ModuleList([AffineCouplingLayer(input_dim, hidden_dim) for _ in range(num_layers)])\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            self.layers.append(AffineCouplingLayer(input_dim, hidden_dim))\n",
    "            self.layers.append(nn.BatchNorm1d(input_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        log_det_jacobian = 0\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, AffineCouplingLayer):\n",
    "                x, scale = layer(x)\n",
    "                print(f\"scale:{scale}\")\n",
    "                log_det_jacobian += scale.sum(dim=1)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        return x, log_det_jacobian\n",
    "\n",
    "    def inverse(self, z): \n",
    "        for layer in reversed(self.layers):\n",
    "            if isintance(layer, AffineCouplingLayer):\n",
    "                z = layer.inverse(z)\n",
    "            else: \n",
    "                z = layer(z)\n",
    "        return z\n",
    "\n",
    "\n",
    "class DNAOrigamiDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        # List of configurations: coordinates and angles\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data[idx], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cc371c2-97b8-4d59-8b43-157c7759baf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class AffineCouplingLayer(nn.Module):\n",
    "#     def __init__(self, input_dim, hidden_dim=512):\n",
    "#         super(AffineCouplingLayer, self).__init__()\n",
    "#         split_dim = input_dim // 2\n",
    "#         self.hidden_dim = hidden_dim\n",
    "\n",
    "#         # Define the scale and translate networks\n",
    "#         self.scale_net = nn.Sequential(\n",
    "#             nn.Linear(split_dim, hidden_dim),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(hidden_dim, hidden_dim),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(hidden_dim, split_dim)\n",
    "#         )\n",
    "\n",
    "#         self.translate_net = nn.Sequential(\n",
    "#             nn.Linear(split_dim, hidden_dim),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(hidden_dim, hidden_dim),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(hidden_dim, split_dim)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         split_dim = x.size(1) // 2\n",
    "#         x1 = x[:, :split_dim]\n",
    "#         x2 = x[:, split_dim:]\n",
    "\n",
    "#         # Apply scale and translation\n",
    "#         scale = self.scale_net(x1)\n",
    "#         translate = self.translate_net(x1)\n",
    "#         scale = torch.clamp(scale, min=-5.0, max=5.0)\n",
    "\n",
    "#         # Affine transformation\n",
    "#         z2 = x2 * torch.exp(scale) + translate\n",
    "#         return torch.cat([x1, z2], dim=1), scale\n",
    "\n",
    "#     def inverse(self, z):\n",
    "#         split_dim = z.size(1) // 2\n",
    "#         z1 = z[:, :split_dim]\n",
    "#         z2 = z[:, split_dim:]\n",
    "\n",
    "#         # Apply inverse scale and translation\n",
    "#         scale = self.scale_net(z1)\n",
    "#         translate = self.translate_net(z1)\n",
    "#         scale = torch.clamp(scale, min=-5.0, max=5.0)\n",
    "\n",
    "#         x2 = (z2 - translate) * torch.exp(-scale)\n",
    "#         return torch.cat([z1, x2], dim=1)\n",
    "\n",
    "# class RealNVP(nn.Module):\n",
    "#     def __init__(self, input_dim, hidden_dim, num_layers):\n",
    "#         super(RealNVP, self).__init__()\n",
    "#         self.layers = nn.ModuleList()\n",
    "#         for i in range(num_layers):\n",
    "#             self.layers.append(AffineCouplingLayer(input_dim, hidden_dim))\n",
    "#             # batch normalization for numerical stability\n",
    "#             self.layers.append(nn.BatchNorm1d(input_dim))\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         log_det_jacobian = 0\n",
    "#         for layer in self.layers:\n",
    "#             if isinstance(layer, AffineCouplingLayer):\n",
    "#                 x, scale = layer(x)\n",
    "#                 log_det_jacobian += scale.sum(dim=1)\n",
    "#             else:\n",
    "#                 x = layer(x)\n",
    "#         return x, log_det_jacobian\n",
    "\n",
    "#     def inverse(self, z):\n",
    "#         for layer in reversed(self.layers):\n",
    "#             if isinstance(layer, AffineCouplingLayer):\n",
    "#                 z = layer.inverse(z)\n",
    "#             else:\n",
    "#                 z = layer(z)\n",
    "#         return z\n",
    "\n",
    "# class DNAOrigamiDataset(Dataset):\n",
    "#     def __init__(self, data):\n",
    "#         # List of configurations: coordinates and angles\n",
    "#         self.data = data\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         return torch.tensor(self.data[idx], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "259134a0-187f-4b32-a8b2-404e66b61520",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_angles(x):\n",
    "    \"\"\"\n",
    "    Computes the angles between consecutive nucleotides in the configuration.\n",
    "    Args:\n",
    "        x: Tensor of shape (batch_size, num_nucleotides, dim), where dim is usually 3, i.e. (x, y, z)\n",
    "\n",
    "    Returns:\n",
    "        Angles: Tensor of angles between three consecutive nucleotides.\n",
    "    \"\"\"\n",
    "    # Vector between nucleotide i and i+1\n",
    "    v1 = x[:, :-2] - x[:, 1:-1]\n",
    "    # Vector between nucleotide i+1 and i+2\n",
    "    v2 = x[:, 1:-1] - x[:, 2:]\n",
    "\n",
    "    # Compute the angle between vectors v1 and v2 using dot product and norm\n",
    "    v1_norm = torch.norm(v1, dim=-1)\n",
    "    v2_norm = torch.norm(v2, dim=-1)\n",
    "\n",
    "    dot_prod = (v1 * v2).sum(dim=-1)\n",
    "    cos_theta = dot_prod / (v1_norm * v2_norm * 1e-9)\n",
    "    cos_theta = torch.clamp(cos_theta, -1.0, 1.0)  # Clamp to avoid invalid values in acos\n",
    "    # angles in radians\n",
    "    angles = torch.acos(cos_theta)\n",
    "\n",
    "    return angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66b7139b-fd0d-4c81-b74f-a63500c7fac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epotential_backbone(x, R0=1.5, k=30):\n",
    "    # Potential based on dist between nucleotides; R0: max bond length, k: spring constant\n",
    "\n",
    "    # Dist between adjacent nucleotides\n",
    "    r_ij = torch.norm(x[:, :-1] - x[:, 1:], dim=-1) + 1e-9\n",
    "    valid_mask = r_ij < R0  # Ensure distances are within valid range for log\n",
    "    r_ij = torch.clamp(r_ij, max=R0 - 1e-6)  # Avoid log of zero or negative values\n",
    "    return (k / 2) * R0**2 * torch.log(1 - (r_ij**2) / R0**2)[valid_mask].sum()\n",
    "    # return (k/2) * R0**2 * torch.log(1 - (r_ij**2) / R0**2).sum()\n",
    "\n",
    "\n",
    "def epotential_stacking(x, epsilon_stack=1.0):\n",
    "    # Stacking potential for sequence-dependant interactions\n",
    "\n",
    "    # Dist between adjacent bases\n",
    "    r_ij = torch.norm(x[:, :-1] - x[:, 1:], dim=-1)\n",
    "\n",
    "    # Angle between consecutive nucleotides\n",
    "    theta = calc_angles(x)\n",
    "\n",
    "    # Dist and Angle dependent terms\n",
    "    f1 = torch.exp(-1 * r_ij**2)\n",
    "    f2 = torch.cos(theta)\n",
    "\n",
    "    return -1 * epsilon_stack * (f1 * f2).sum()\n",
    "\n",
    "\n",
    "def hydrogen_bonding(x, base_pairs, epsilon_hb=2.0):\n",
    "    # Hydrogen bonding between complementary base pairs\n",
    "    hb_energy = 0\n",
    "    batch_size = x.size(0)\n",
    "\n",
    "    for i, j in base_pairs:\n",
    "        if i < x.size(1) and j < x.size(1): # Ensure indices are within bounds\n",
    "            # Dist between paired nucleotides\n",
    "            r_ij = torch.norm(x[:, i] - x[:, j], dim=-1)\n",
    "            r_ij = torch.clamp(r_ij, min=1e-3)  # Avoid extremely small distances\n",
    "            # Angle between bases\n",
    "            theta_ij = calc_angles(torch.stack([x[:, i], x[:, j]], dim=1).view(batch_size, -1, 3))\n",
    "    \n",
    "            hb_energy += -1 * epsilon_hb * torch.exp(-1*r_ij) * torch.cos(theta_ij)\n",
    "\n",
    "    return hb_energy\n",
    "\n",
    "\n",
    "def excluded_volume(x, A=5.0, lambda_val=1.0):\n",
    "    # Excluded volume to prevent nucleotides from overlapping\n",
    "    r_ij = torch.norm(x[:, :, None] - x[:, None, :], dim=-1)  # Pairwise distances\n",
    "\n",
    "    return A * torch.exp(-1*r_ij / lambda_val).sum()\n",
    "\n",
    "\n",
    "def coaxial_stacking(x, epsilon_coaxial=1.5):\n",
    "    # Coaxial stacking interaction, important for junctions and nicks\n",
    "    r_ij = torch.norm(x[:, :-1] - x[:, 1:], dim=-1)\n",
    "    theta_ij = calc_angles(x) # calc angles at junctions\n",
    "\n",
    "    return -1 * epsilon_coaxial * torch.exp(-1*r_ij) * torch.cos(theta_ij).sum()\n",
    "\n",
    "\n",
    "def entropy_loss(x, scaffold_loops, kuhn_length=1.0):\n",
    "    \"\"\"\n",
    "    Calculate the entropy loss based on scaffold loop configurations.\n",
    "    Args:\n",
    "        x: Tensor of shape (batch_size, num_nucleotides, dim), representing the coordinates of the DNA scaffold.\n",
    "        scaffold_loops: List of tuples indicating start and end indices of each loop.\n",
    "        kuhn_length: Approximate length of a Kuhn segment.\n",
    "\n",
    "    Returns:\n",
    "        Entropy: Scalar value representing the total entropy contribution for the given configuration.\n",
    "    \"\"\"\n",
    "    R = 8.314  # Gas constant in J/(mol*K)\n",
    "    total_entropy = 0.0\n",
    "\n",
    "    for loop_start, loop_end in scaffold_loops:\n",
    "        loop_length = loop_end - loop_start\n",
    "        if loop_length > 1:\n",
    "            # Calculate end-to-end vector r\n",
    "            p_start = x[:, loop_start, :]\n",
    "            p_end = x[:, loop_end, :]\n",
    "            r = torch.norm(p_end - p_start, dim=-1).mean()  # Average over batch\n",
    "\n",
    "            # Calculate num of Kuhn segments\n",
    "            N = loop_length / kuhn_length\n",
    "            loop_entropy = -R * math.log(N + 1e-9)  # avoid log(0)\n",
    "            total_entropy += loop_entropy\n",
    "\n",
    "    return total_entropy\n",
    "\n",
    "\n",
    "# Cal Gibbs Free energy\n",
    "def dna_energy_func(x, base_pairs, scaffold_loops, temperature=300):\n",
    "    # Existing energy calculations\n",
    "    e_backbone = epotential_backbone(x)\n",
    "    e_stacking = epotential_stacking(x)\n",
    "    e_hb = hydrogen_bonding(x, base_pairs)\n",
    "    e_excluded = excluded_volume(x)\n",
    "    e_coaxial = coaxial_stacking(x)\n",
    "\n",
    "    # Calculate entropy loss\n",
    "    s_entropy = entropy_loss(x, scaffold_loops)\n",
    "\n",
    "    # Gibbs Free Energy: G = H - TS\n",
    "    g_total = e_backbone + e_stacking + e_hb + e_excluded + e_coaxial - temperature * s_entropy\n",
    "    \n",
    "    return g_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50f31a53-67a4-43c4-b0f5-8ada7cdb1920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KL Divergence Loss with Energy Function\n",
    "def kl_divergence_loss(model, data, base_pairs, scaffold_loops, energy_func, temperature=300):\n",
    "    x = model(data)\n",
    "    z, log_det_jacobian = model.forward(data)  # Use the invertible property of Boltzmann generators to obtain z and Jacobian\n",
    "    energy = energy_func(x, base_pairs, scaffold_loops, temperature)\n",
    "    kl_loss = (energy - log_det_jacobian).mean()  # KL divergence with Jacobian correction\n",
    "    if torch.isnan(kl_loss):\n",
    "        print(\"NaN detected in KL loss!\")\n",
    "        print(f\"Energy: {energy}\")\n",
    "        \n",
    "    return kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a404b26-41fa-4a33-856e-4a599bf70a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def monitor_gradients(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        # print(f\"Layer {name}; Gradient norm = {param.grad.norm()}\")\n",
    "        if param.grad is not None:\n",
    "            print(f\"Layer {name}; Gradient norm = {param.grad.norm()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3a5e021-b88f-4dff-8600-f83d8e2b0239",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            torch.nn.init.zeros_(m.bias) # init biases to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83125667-4342-4f2c-8dd7-4a91f2c44bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_scaffold_loops(json_data):\n",
    "    \"\"\"\n",
    "    Identifies scaffold loops in the DNA configuration based on base pairing and position data.\n",
    "    \n",
    "    Args:\n",
    "        json_data: Parsed JSON data containing systems, strands, and monomer information.\n",
    "    \n",
    "    Returns:\n",
    "        scaffold_loops: List of tuples representing (loop_start, loop_end) indices for each identified loop.\n",
    "    \"\"\"\n",
    "    scaffold_loops = []\n",
    "    monomer_data_map = {}  # Keep track of monomer data, including their index and position\n",
    "    idx = 0\n",
    "\n",
    "    for system in json_data['systems']:\n",
    "        for strand in system['strands']:\n",
    "            for monomer in strand['monomers']:\n",
    "                monomer_data_map[monomer['id']] = {\n",
    "                    'index': idx,\n",
    "                    'p': monomer['p']\n",
    "                }\n",
    "                idx += 1\n",
    "\n",
    "    # Reset index for processing loop identification\n",
    "    idx = 0\n",
    "    for system in json_data['systems']:\n",
    "        for strand in system['strands']:\n",
    "            for monomer in strand['monomers']:\n",
    "                if 'bp' in monomer:\n",
    "                    paired_id = monomer['bp']\n",
    "                    if paired_id in monomer_data_map:\n",
    "                        paired_idx = monomer_data_map[paired_id]['index']\n",
    "                        # Identify if paired_idx is before idx, which may indicate a loop\n",
    "                        if paired_idx > idx:\n",
    "                            scaffold_loops.append((idx, paired_idx))\n",
    "                        # if paired_idx < idx:\n",
    "                        #     loop_start = paired_idx\n",
    "                        #     loop_end = idx\n",
    "                        #     # Calculate distance between paired nucleotides\n",
    "                        #     p_start = torch.tensor(monomer_data_map[paired_id]['p'])\n",
    "                        #     p_end = torch.tensor(monomer['p'])\n",
    "                        #     distance = torch.norm(p_end - p_start).item()\n",
    "                        #     # threshold to confirm if it's a loop\n",
    "                        #     if distance < 5.0:  # Arbitrary threshold for identifying spatial proximity\n",
    "                        #         scaffold_loops.append((loop_start, loop_end))\n",
    "                idx += 1\n",
    "\n",
    "    return scaffold_loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62a14e04-cc20-4ade-b348-e4d1510d4150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boltzmann generator training\n",
    "def train(model, dataloader, optimizer, scheduler, base_pairs, scaffold_loops, dna_energy_func, epochs, temperature=300):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for x in dataloader:\n",
    "            # moving data to gpu\n",
    "            x = x.to(device)\n",
    "            # base_pairs = [(i, j) for i, j in base_pairs]\n",
    "            # Add gaussian noise to input data\n",
    "            noise = torch.randn_like(x) * 0.01\n",
    "            x += noise\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            # Calc KL divergence loss\n",
    "            loss = kl_divergence_loss(model, x, base_pairs, scaffold_loops, dna_energy_func, temperature)\n",
    "            monitor_gradients(model)\n",
    "            if torch.isnan(loss):\n",
    "                continue \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "            optimizer.step()\n",
    "            # monitor_gradients(model)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        scheduler.step(total_loss/len(dataloader))\n",
    "        print(f\"Epoch {epoch+1}/{epochs} -> Loss: {total_loss / len(dataloader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70fbb8ed-700a-419f-951b-099f287070c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling new DNA configurations from latent space\n",
    "def sample_dna_configs(model, num_samples, input_dim):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Latent space dimension should match the input dimension of the RealNVP model\n",
    "        # latent_dim = model.layers[0].scale_net[0].in_features * 2\n",
    "        #z = torch.randn(num_samples, latent_dim, device=device)\n",
    "\n",
    "        z = torch.randn(num_samples, input_dim).to(device)\n",
    "        x_samples = model.inverse(z)\n",
    "\n",
    "    return x_samples.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "525daa43-0572-4cf6-8afa-e6a96d8b9e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Randomly generated data for demo\n",
    "input_dim = 9\n",
    "hidden_dim = 256\n",
    "num_layers = 10\n",
    "learning_rate = 0.0001\n",
    "temperature = 300\n",
    "batch_size = 16\n",
    "epochs = 50\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00067075-e866-4409-8b42-a35d405146a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"./data/output.oxview\", 'r') as file:\n",
    "#     json_data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb0c56f6-7a5e-435d-b549-cdee5bb3d618",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_data(df, target_length=None):\n",
    "    if target_length is None:\n",
    "        target_length = max([len(x) for x in df])\n",
    "\n",
    "    padded_data_list = []\n",
    "    for x in df:\n",
    "        current_length = len(x)\n",
    "        if current_length < target_length:\n",
    "            pad_length = target_length - current_length\n",
    "            padding = np.zeros((pad_length, x.shape[1]))\n",
    "            padded_data = np.vstack([x, padding])\n",
    "        else:\n",
    "            padded_data = x\n",
    "\n",
    "        padded_data_list.append(padded_data)\n",
    "\n",
    "    return padded_data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1785b12d-da07-4746-bcf6-5044306b4599",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './data'\n",
    "data_list = []\n",
    "base_pair_list = []\n",
    "scaffold_loops_list = []\n",
    "\n",
    "for filename in os.listdir(data_dir):\n",
    "    if filename.endswith(\".oxview\") or filename.endswith(\".json\"):\n",
    "        with open(os.path.join(data_dir, filename), 'r') as f:\n",
    "            json_data = json.load(f)\n",
    "            position_data = []\n",
    "            base_pairs = []\n",
    "            scaffolds_loops = []\n",
    "            monomer_index_map = {} # Map to keep track of monomer IDs and their indices in the dataset\n",
    "\n",
    "            idx = 0\n",
    "            edge_index = []       \n",
    "            for system in json_data['systems']:\n",
    "                for strand in system['strands']:\n",
    "                    for monomer in strand['monomers']:\n",
    "                        p = monomer['p'] + monomer['a1'] + monomer['a3']\n",
    "                        position_data.append(p)\n",
    "                        monomer_index_map[monomer['id']] = idx\n",
    "                        idx += 1\n",
    "                        if 'bp' in monomer and monomer['bp'] in monomer_index_map:\n",
    "                            base_pairs.append((monomer_index_map[monomer['id']], monomer_index_map[monomer['bp']]))\n",
    "\n",
    "            scaffold_loops = identify_scaffold_loops(json_data)\n",
    "            data_list.append(np.array(position_data))\n",
    "            base_pair_list.append(base_pairs)\n",
    "            scaffold_loops_list.append(scaffolds_loops)\n",
    "\n",
    "data_list = pad_data(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae329aa1-bbc6-4c1a-91b7-e193a703e5f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dir:/home/sanbaras/sulcLab/origamiModels\n"
     ]
    }
   ],
   "source": [
    "print(f\"dir:{os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cbffbfae-92dc-4e35-944e-7ce173af2977",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = [torch.randn(1, input_dim) for _ in range(100)] # we will replace with actual dna coords/oxDNA datasets\n",
    "# data = np.random.randn(1000, input_dim)\n",
    "dataset = DNAOrigamiDataset(data_list)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e7eed5b5-449d-4833-b571-5d05e5b44eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden dim256\n",
      "hidden dim256\n",
      "hidden dim256\n",
      "hidden dim256\n",
      "hidden dim256\n",
      "hidden dim256\n",
      "hidden dim256\n",
      "hidden dim256\n",
      "hidden dim256\n",
      "hidden dim256\n",
      "split dim:8117\n",
      "xtorch.Size([2, 16235, 9])\n",
      "x1torch.Size([2, 8118, 9])\n",
      "x2torch.Size([2, 8117, 9])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (16236x9 and 5x256)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mReduceLROnPlateau(optimizer, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m, factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrealnvp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_pair_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaffold_loops_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 16\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, optimizer, scheduler, base_pairs, scaffold_loops, dna_energy_func, epochs, temperature)\u001b[0m\n\u001b[1;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Calc KL divergence loss\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mkl_divergence_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_pairs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaffold_loops\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdna_energy_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m monitor_gradients(model)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misnan(loss):\n",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m, in \u001b[0;36mkl_divergence_loss\u001b[0;34m(model, data, base_pairs, scaffold_loops, energy_func, temperature)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mkl_divergence_loss\u001b[39m(model, data, base_pairs, scaffold_loops, energy_func, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m):\n\u001b[0;32m----> 3\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     z, log_det_jacobian \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward(data)  \u001b[38;5;66;03m# Use the invertible property of Boltzmann generators to obtain z and Jacobian\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     energy \u001b[38;5;241m=\u001b[39m energy_func(x, base_pairs, scaffold_loops, temperature)\n",
      "File \u001b[0;32m/packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 16\u001b[0m, in \u001b[0;36mRealNVP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(layer, AffineCouplingLayer):\n\u001b[0;32m---> 16\u001b[0m         x, scale \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscale:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscale\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m         log_det_jacobian \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m scale\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 38\u001b[0m, in \u001b[0;36mAffineCouplingLayer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx2\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx2\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# x1, x2 = x.chunk(2, dim=1)\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# print(f\"x1 shape: {x1.shape}, x2 shape: {x2.shape}\")\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Scale network with skip connection\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m hidden \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_net_fc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     39\u001b[0m hidden \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_net_fc2(hidden))\u001b[38;5;241m+\u001b[39mhidden \u001b[38;5;66;03m# skip connection\u001b[39;00m\n\u001b[1;32m     40\u001b[0m scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_net_fc3(hidden)\n",
      "File \u001b[0;32m/packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (16236x9 and 5x256)"
     ]
    }
   ],
   "source": [
    " # Model, optimizer, and training\n",
    "realnvp = RealNVP(input_dim, hidden_dim, num_layers).to(device)\n",
    "# realnvp.apply(initialize_weights)\n",
    "optimizer = optim.Adam(realnvp.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "\n",
    "# Train the model\n",
    "train(realnvp, dataloader, optimizer, scheduler, base_pair_list, scaffold_loops_list, epochs, temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca44a302-e6dc-47ac-b72d-2b8efb87bb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dna_configs = sample_dna_configs(realnvp, num_samples=10, input_dim=input_dim)\n",
    "print(new_dna_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b91772-ccb0-4be9-a7b9-7c2c0860906a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu-2.1.0-cuda-12.1",
   "language": "python",
   "name": "pytorch-gpu-2.1.0-cuda-12.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
